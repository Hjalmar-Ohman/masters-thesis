{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. INSTALL / IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olofswedberg/Documents/thesis/masters-thesis/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # TODO: This can introduce misscalculations. Opt for just CPU or GPU.\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # This is to avoid conflicts with Faiss (for MAC users)\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from openai import OpenAI\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# In your notebook (example):\n",
    "from common_utils import embed_texts, embed_images, encode_image_to_base64, search_index, retrieve_context, call_gpt_4, extract_figures_from_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CONFIGURE OPENAI & OTHER SETUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")  # Make sure your OpenAI key is set\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create a cache dictionary in memory\n",
    "# response_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PROCESS THE PDF (TEXT + IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4 images.\n"
     ]
    }
   ],
   "source": [
    "PDF_FILE = \"../knowledge/catsanddogs.pdf\"\n",
    "\n",
    "text_data = []\n",
    "image_data = []\n",
    "\n",
    "# Extract text\n",
    "reader = PdfReader(PDF_FILE)\n",
    "num_pages = len(reader.pages)\n",
    "\n",
    "for page_i in range(num_pages):\n",
    "    page = reader.pages[page_i]\n",
    "    page_text = page.extract_text()\n",
    "    \n",
    "    if page_text and page_text.strip():\n",
    "        text_data.append({\n",
    "            \"text\": page_text.strip(),\n",
    "            \"page_number\": page_i + 1\n",
    "        })\n",
    "\n",
    "# Extract images\n",
    "all_images = extract_figures_from_pdf(PDF_FILE)\n",
    "print(f\"Extracted {len(all_images)} images.\")\n",
    "\n",
    "for i, pil_img in enumerate(all_images):\n",
    "    image_data.append({\n",
    "        \"image\": pil_img,\n",
    "        \"image_number\": i + 1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CREATE EMBEDDINGS FOR TEXT AND IMAGE CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metadata = []\n",
    "all_embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5A. Embed all text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_list = [td[\"text\"] for td in text_data]\n",
    "if len(texts_list) > 0:\n",
    "    text_embeddings = embed_texts(texts_list, clip_processor, clip_model)\n",
    "    for i, emb in enumerate(text_embeddings):\n",
    "        all_metadata.append({\n",
    "            \"type\": \"text\",\n",
    "            \"content\": text_data[i][\"text\"],\n",
    "            \"page_number\": text_data[i][\"page_number\"]\n",
    "        })\n",
    "        all_embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5B. Embed all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_images_list = [id_[\"image\"] for id_ in image_data]\n",
    "if len(pil_images_list) > 0:\n",
    "    image_embeddings = embed_images(pil_images_list, clip_processor, clip_model)\n",
    "    for i, emb in enumerate(image_embeddings):\n",
    "        \n",
    "        # Convert PIL image to base64 once (so we can send it to GPT-4 with Vision)\n",
    "        base64_str = encode_image_to_base64(image_data[i][\"image\"])\n",
    "        \n",
    "        all_metadata.append({\n",
    "            \"type\": \"image\",\n",
    "            # Store the base64 data directly as \"content\"\n",
    "            \"content\": base64_str,\n",
    "            \"image_number\": image_data[i][\"image_number\"]\n",
    "        })\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "# Convert to NumPy array\n",
    "all_embeddings = np.array(all_embeddings).astype('float32')\n",
    "embedding_dimension = all_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. BUILD & POPULATE FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and return a flat Faiss index of the specified dimension.\n",
    "index = faiss.IndexFlatIP(embedding_dimension)\n",
    "\n",
    "# Add embeddings to a Faiss index.\n",
    "index.add(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. QUERY PIPELINE (RETRIEVAL + GENERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(user_query, top_k=3):\n",
    "    \"\"\"\n",
    "    1. Embed the user query (as text).\n",
    "    2. Retrieve top_k similar items from the PDF (text or image).\n",
    "    3. Build a ChatCompletion messages list with text + images.\n",
    "    4. Return GPT-4's answer.\n",
    "    \"\"\"\n",
    "    # Step 1: Embed user query\n",
    "    query_emb = embed_texts([user_query], clip_processor, clip_model)  # shape: (1, D)\n",
    "    \n",
    "    # Step 2: Retrieve from Faiss\n",
    "    distances, faiss_indices = search_index(index, query_emb, top_k=top_k)\n",
    "    retrieved_items = retrieve_context(faiss_indices, all_metadata)\n",
    "    print(\"Distances:\", distances)\n",
    "\n",
    "    # Step 3: Build the messages payload\n",
    "    # We'll pass the user's question as the first part of the content,\n",
    "    # then each retrieved item (text or image) as separate parts.\n",
    "    user_content = []\n",
    "    \n",
    "    # Add user query\n",
    "    user_content.append({\"type\": \"text\", \"text\": f\"User query: {user_query}\"})\n",
    "    \n",
    "    # Add each retrieved item\n",
    "    for item in retrieved_items:\n",
    "        if item[\"type\"] == \"text\":\n",
    "            # Provide textual snippet\n",
    "            user_content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"(page {item['page_number']}) {item['content'][:500]}...\"\n",
    "            })\n",
    "        elif item[\"type\"] == \"image\":\n",
    "            # Provide the base64 image data as a data URI\n",
    "            base64_str = item[\"content\"]\n",
    "            user_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{base64_str}\"\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # Step 4: Call GPT-4 with the full message payload\n",
    "    answer = call_gpt_4(user_content)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. EXAMPLE USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[0.6136132  0.4135872  0.29333058 0.29129386]]\n",
      "\n",
      "Q: What colors are the wheels on the skateboards?\n",
      "A: The wheels on the skateboards in the images are:\n",
      "\n",
      "1. The first skateboard has silver wheels.\n",
      "2. The second skateboard has orange wheels.\n"
     ]
    }
   ],
   "source": [
    "user_query_1 = \"What colors are the wheels on the skateboards?\"\n",
    "response_1 = answer_query(user_query_1, top_k=4)\n",
    "print(\"\\nQ:\", user_query_1)\n",
    "print(\"A:\", response_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Defining the RAG as a Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from common_utils import embed_texts, embed_images, encode_image_to_base64, search_index, retrieve_context, call_gpt_4, extract_figures_from_pdf\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, pdf_path, openai_api_key):\n",
    "        self.openai_client = OpenAI(api_key=openai_api_key)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_id = \"openai/clip-vit-base-patch32\"\n",
    "        self.clip_model = CLIPModel.from_pretrained(self.model_id).to(self.device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(self.model_id)\n",
    "        self.pdf_path = pdf_path\n",
    "        self.index = None\n",
    "        self.all_metadata = []\n",
    "        self._process_pdf()\n",
    "        self._build_faiss()\n",
    "    \n",
    "    def _process_pdf(self):\n",
    "        text_data, image_data = [], []\n",
    "        reader = PdfReader(self.pdf_path)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text and page_text.strip():\n",
    "                text_data.append({\"text\": page_text.strip(), \"page_number\": i + 1})\n",
    "        \n",
    "        all_images = extract_figures_from_pdf(self.pdf_path)\n",
    "        for i, pil_img in enumerate(all_images):\n",
    "            image_data.append({\"image\": pil_img, \"image_number\": i + 1})\n",
    "        \n",
    "        texts_list = [td[\"text\"] for td in text_data]\n",
    "        if texts_list:\n",
    "            text_embeddings = embed_texts(texts_list, self.clip_processor, self.clip_model)\n",
    "            for i, emb in enumerate(text_embeddings):\n",
    "                self.all_metadata.append({\"type\": \"text\", \"content\": text_data[i][\"text\"], \"page_number\": text_data[i][\"page_number\"]})\n",
    "        \n",
    "        pil_images_list = [id_[\"image\"] for id_ in image_data]\n",
    "        if pil_images_list:\n",
    "            image_embeddings = embed_images(pil_images_list, self.clip_processor, self.clip_model)\n",
    "            for i, emb in enumerate(image_embeddings):\n",
    "                base64_str = encode_image_to_base64(image_data[i][\"image\"])\n",
    "                self.all_metadata.append({\"type\": \"image\", \"content\": base64_str, \"image_number\": image_data[i][\"image_number\"]})\n",
    "        \n",
    "        self.all_embeddings = np.array(text_embeddings + image_embeddings).astype(\"float32\")\n",
    "        self.embedding_dimension = self.all_embeddings.shape[1]\n",
    "    \n",
    "    def _build_faiss(self):\n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dimension)\n",
    "        self.index.add(self.all_embeddings)\n",
    "    \n",
    "    def get_most_relevant_docs(self, user_query, top_k=3):\n",
    "        query_emb = embed_texts([user_query], self.clip_processor, self.clip_model)\n",
    "        distances, faiss_indices = search_index(self.index, query_emb, top_k=top_k)\n",
    "        return retrieve_context(faiss_indices, self.all_metadata)\n",
    "    \n",
    "    def generate_answer(self, user_query, retrieved_docs):\n",
    "        user_content = [{\"type\": \"text\", \"text\": f\"User query: {user_query}\"}]\n",
    "        for doc in retrieved_docs:\n",
    "            if doc[\"type\"] == \"text\":\n",
    "                user_content.append({\"type\": \"text\", \"text\": f\"(page {doc['page_number']}) {doc['content'][:500]}...\"})\n",
    "            elif doc[\"type\"] == \"image\":\n",
    "                user_content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{doc['content']}\"}})\n",
    "        return call_gpt_4(user_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0 (main, Oct  7 2024, 05:02:14) [Clang 15.0.0 (clang-1500.1.0.2.5)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "34d35bb7a41995741358303864c0abd463933b7f4684d12ed372b9f8e1810ab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
