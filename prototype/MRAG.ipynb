{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. INSTALL / IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # TODO: This can introduce misscalculations. Opt for just CPU or GPU.\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\" # This is to avoid conflicts with Faiss (for MAC users)\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from PIL import Image\n",
    "\n",
    "# In your notebook (example):\n",
    "from common_utils import generate_image_summary ,encode_image_to_base64, search_index, retrieve_context, extract_rasterized_images_from_pdf, call_gpt_4\n",
    "from multimodalembedder import create_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Embedder and PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"../knowledge/subset_monetary_policy_report.pdf\"\n",
    "EMBEDDER = \"FLAVA\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedder = create_embedder(EMBEDDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PROCESS THE PDF (TEXT + IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "image_data = []\n",
    "\n",
    "# Extract text\n",
    "reader = PdfReader(PDF_FILE)\n",
    "num_pages = len(reader.pages)\n",
    "\n",
    "for page_i in range(num_pages):\n",
    "    page = reader.pages[page_i]\n",
    "    page_text = page.extract_text()\n",
    "    \n",
    "    if page_text and page_text.strip():\n",
    "        text_data.append({\n",
    "            \"text\": page_text.strip(),\n",
    "            \"page_number\": page_i + 1\n",
    "        })\n",
    "\n",
    "# Extract images\n",
    "all_image_paths = extract_rasterized_images_from_pdf(PDF_FILE)\n",
    "all_images = []\n",
    "for image_info in all_image_paths:\n",
    "    image_path = image_info[\"image_path\"]\n",
    "\n",
    "    # Convert the image to base64 for processing\n",
    "    image = Image.open(image_path)\n",
    "    all_images.append(image)\n",
    "\n",
    "for i, pil_img in enumerate(all_images):\n",
    "    image_data.append({\n",
    "        \"image\": pil_img,\n",
    "        \"image_number\": i + 1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CREATE EMBEDDINGS FOR TEXT AND IMAGE CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metadata = []\n",
    "all_embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5A. Embed all text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_list = [td[\"text\"] for td in text_data]\n",
    "if len(texts_list) > 0:\n",
    "    text_embeddings = embedder.embed_text(texts_list)\n",
    "    for i, emb in enumerate(text_embeddings):\n",
    "        all_metadata.append({\n",
    "            \"type\": \"text\",\n",
    "            \"content\": text_data[i][\"text\"],\n",
    "            \"page_number\": text_data[i][\"page_number\"]\n",
    "        })\n",
    "        all_embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5B. Embed all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_images_list = [id_[\"image\"] for id_ in image_data]\n",
    "if len(pil_images_list) > 0:\n",
    "    image_embeddings = embedder.embed_images(pil_images_list)\n",
    "    for i, emb in enumerate(image_embeddings):\n",
    "        \n",
    "        # Convert PIL image to base64 once (so we can send it to GPT-4 with Vision)\n",
    "        base64_str = encode_image_to_base64(image_data[i][\"image\"])\n",
    "        all_metadata.append({\n",
    "            \"type\": \"image\",\n",
    "            \"content\": base64_str,\n",
    "            \"image_number\": image_data[i][\"image_number\"]\n",
    "        })\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "# Convert to NumPy array\n",
    "all_embeddings = np.array(all_embeddings).astype('float32')\n",
    "embedding_dimension = all_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. BUILD & POPULATE FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and return a flat Faiss index of the specified dimension.\n",
    "index = faiss.IndexFlatIP(embedding_dimension)\n",
    "\n",
    "# Add embeddings to a Faiss index.\n",
    "index.add(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. QUERY PIPELINE (RETRIEVAL + GENERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(user_query, top_k=3):\n",
    "    \"\"\"\n",
    "    1. Embed the user query (as text).\n",
    "    2. Retrieve top_k similar items from the PDF (text or image).\n",
    "    3. Build a ChatCompletion messages list with text + images.\n",
    "    4. Return GPT-4's answer.\n",
    "    \"\"\"\n",
    "    # Step 1: Embed user query\n",
    "    query_emb = embedder.embed_text([user_query])  # shape: (1, D)\n",
    "    \n",
    "    # Step 2: Retrieve from Faiss\n",
    "    distances, faiss_indices = search_index(index, query_emb, top_k=top_k)\n",
    "    retrieved_items = retrieve_context(faiss_indices, all_metadata)\n",
    "    \n",
    "    # Print distances and retrieved items in pairs\n",
    "    for distance, item in zip(distances[0], retrieved_items):\n",
    "        print(f\"Distance: {distance}, Item: {item}\\n\")\n",
    "\n",
    "    # Step 3: Build the messages payload\n",
    "    # We'll pass the user's question as the first part of the content,\n",
    "    # then each retrieved item (text or image) as separate parts.\n",
    "    user_content = []\n",
    "    \n",
    "    # Add user query\n",
    "    user_content.append({\"type\": \"text\", \"text\": f\"User query: {user_query}\"})\n",
    "    \n",
    "    # Add each retrieved item\n",
    "    for item in retrieved_items:\n",
    "        if item[\"type\"] == \"text\":\n",
    "            continue\n",
    "            # Provide textual snippet\n",
    "            user_content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"(page {item['page_number']}) {item['content'][:500]}...\"\n",
    "            })\n",
    "        elif item[\"type\"] == \"image\":\n",
    "            # Provide the base64 image data as a data URI\n",
    "            base64_str = item[\"content\"]\n",
    "            user_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{base64_str}\"\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # Step 4: Call GPT-4 with the full message payload\n",
    "    answer = \"call_gpt_4(user_content)\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. EXAMPLE USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_1 = \"Figure 9. Non-financial companies' debts SEK billion 9000 Loans from banks, etc. Trade credits and advances debt securities other Note. Refers to Swedish non-financial companies, including tenant-owner housing associations.\"\n",
    "response_1 = answer_query(user_query_1, top_k=100)\n",
    "print(\"\\nQ:\", user_query_1)\n",
    "print(\"A:\", response_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
