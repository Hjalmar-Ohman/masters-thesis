{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. INSTALL / IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # TODO: This can introduce misscalculations. Opt for just CPU or GPU.\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\" # This is to avoid conflicts with Faiss (for MAC users)\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from PIL import Image\n",
    "\n",
    "# In your notebook (example):\n",
    "from common_utils import encode_image_to_base64, search_index, retrieve_context, call_gpt_4\n",
    "from multimodalembedder import create_embedder\n",
    "from ipynb.fs.full.evalbook import extract_images_from_pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Embedder and PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56014cfb09b9476683dd5d251b7e6828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Skola\\Master's Thesis\\masters-thesis\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hjalmar Ã–hman\\.cache\\huggingface\\hub\\models--Salesforce--blip2-itm-vit-g. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7233c878a4504c24b3ffa27e498f69c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9080dd66534ba68eea69f94d5587d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a07fd4bda554e3d88e9682ce9c89675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7441e2e0859e406a9e5f58aa7d520464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/263 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60b2273671b483cb71f3a74f0ad057d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/554 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d795306921304639905896e421fc1d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedder = create_embedder(\"CLIP\")\n",
    "\n",
    "PDF_FILE = \"../knowledge/subset_monetary_policy_report.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PROCESS THE PDF (TEXT + IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "image_data = []\n",
    "\n",
    "# Extract text\n",
    "reader = PdfReader(PDF_FILE)\n",
    "num_pages = len(reader.pages)\n",
    "\n",
    "for page_i in range(num_pages):\n",
    "    page = reader.pages[page_i]\n",
    "    page_text = page.extract_text()\n",
    "    \n",
    "    if page_text and page_text.strip():\n",
    "        text_data.append({\n",
    "            \"text\": page_text.strip(),\n",
    "            \"page_number\": page_i + 1\n",
    "        })\n",
    "\n",
    "# Extract images\n",
    "all_image_paths = extract_images_from_pdf(PDF_FILE)\n",
    "all_images = []\n",
    "for image_info in all_image_paths:\n",
    "    image_path = image_info[\"image_path\"]\n",
    "\n",
    "    # Convert the image to base64 for processing\n",
    "    image = Image.open(image_path)\n",
    "    all_images.append(image)\n",
    "\n",
    "for i, pil_img in enumerate(all_images):\n",
    "    image_data.append({\n",
    "        \"image\": pil_img,\n",
    "        \"image_number\": i + 1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CREATE EMBEDDINGS FOR TEXT AND IMAGE CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metadata = []\n",
    "all_embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5A. Embed all text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in BLIP-2 should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Blip2ForImageTextRetrieval.forward() missing 1 required positional argument: 'pixel_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m texts_list \u001b[38;5;241m=\u001b[39m [td[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m td \u001b[38;5;129;01min\u001b[39;00m text_data]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(texts_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m     text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_embeddings):\n\u001b[0;32m      5\u001b[0m         all_metadata\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: text_data[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: text_data[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_number\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m         })\n",
      "File \u001b[1;32mc:\\Skola\\Master's Thesis\\masters-thesis\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Skola\\Master's Thesis\\masters-thesis\\prototype\\multimodalembedder.py:164\u001b[0m, in \u001b[0;36mBLIP2Embedder.embed_text\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    161\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(text\u001b[38;5;241m=\u001b[39mtexts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Pass through the model\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_image_text_matching_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m text_embeds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtext_embeds  \u001b[38;5;66;03m# (batch_size, embedding_dim)\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# L2-normalize and move to CPU as numpy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Skola\\Master's Thesis\\masters-thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Skola\\Master's Thesis\\masters-thesis\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: Blip2ForImageTextRetrieval.forward() missing 1 required positional argument: 'pixel_values'"
     ]
    }
   ],
   "source": [
    "texts_list = [td[\"text\"] for td in text_data]\n",
    "if len(texts_list) > 0:\n",
    "    text_embeddings = embedder.embed_text(texts_list)\n",
    "    for i, emb in enumerate(text_embeddings):\n",
    "        all_metadata.append({\n",
    "            \"type\": \"text\",\n",
    "            \"content\": text_data[i][\"text\"],\n",
    "            \"page_number\": text_data[i][\"page_number\"]\n",
    "        })\n",
    "        all_embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5B. Embed all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_images_list = [id_[\"image\"] for id_ in image_data]\n",
    "if len(pil_images_list) > 0:\n",
    "    image_embeddings = embedder.embed_images(pil_images_list)\n",
    "    for i, emb in enumerate(image_embeddings):\n",
    "        \n",
    "        # Convert PIL image to base64 once (so we can send it to GPT-4 with Vision)\n",
    "        base64_str = encode_image_to_base64(image_data[i][\"image\"])\n",
    "        all_metadata.append({\n",
    "            \"type\": \"image\",\n",
    "            \"content\": base64_str,\n",
    "            \"image_number\": image_data[i][\"image_number\"]\n",
    "        })\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "# Convert to NumPy array\n",
    "all_embeddings = np.array(all_embeddings).astype('float32')\n",
    "embedding_dimension = all_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. BUILD & POPULATE FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and return a flat Faiss index of the specified dimension.\n",
    "index = faiss.IndexFlatIP(embedding_dimension)\n",
    "\n",
    "# Add embeddings to a Faiss index.\n",
    "index.add(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. QUERY PIPELINE (RETRIEVAL + GENERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(user_query, top_k=3):\n",
    "    \"\"\"\n",
    "    1. Embed the user query (as text).\n",
    "    2. Retrieve top_k similar items from the PDF (text or image).\n",
    "    3. Build a ChatCompletion messages list with text + images.\n",
    "    4. Return GPT-4's answer.\n",
    "    \"\"\"\n",
    "    # Step 1: Embed user query\n",
    "    query_emb = embedder.embed_text([user_query])  # shape: (1, D)\n",
    "    \n",
    "    # Step 2: Retrieve from Faiss\n",
    "    distances, faiss_indices = search_index(index, query_emb, top_k=top_k)\n",
    "    retrieved_items = retrieve_context(faiss_indices, all_metadata)\n",
    "    \n",
    "    # Print distances and retrieved items in pairs\n",
    "    for distance, item in zip(distances[0], retrieved_items):\n",
    "        print(f\"Distance: {distance}, Item: {item}\\n\")\n",
    "\n",
    "    # Step 3: Build the messages payload\n",
    "    # We'll pass the user's question as the first part of the content,\n",
    "    # then each retrieved item (text or image) as separate parts.\n",
    "    user_content = []\n",
    "    \n",
    "    # Add user query\n",
    "    user_content.append({\"type\": \"text\", \"text\": f\"User query: {user_query}\"})\n",
    "    \n",
    "    # Add each retrieved item\n",
    "    for item in retrieved_items:\n",
    "        if item[\"type\"] == \"text\":\n",
    "            continue\n",
    "            # Provide textual snippet\n",
    "            user_content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"(page {item['page_number']}) {item['content'][:500]}...\"\n",
    "            })\n",
    "        elif item[\"type\"] == \"image\":\n",
    "            # Provide the base64 image data as a data URI\n",
    "            base64_str = item[\"content\"]\n",
    "            user_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{base64_str}\"\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # Step 4: Call GPT-4 with the full message payload\n",
    "    answer = \"call_gpt_4(user_content)\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. EXAMPLE USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_1 = \"Figure 9. Non-financial companies' debts SEK billion 9000 Loans from banks, etc. Trade credits and advances debt securities other Note. Refers to Swedish non-financial companies, including tenant-owner housing associations.\"\n",
    "response_1 = answer_query(user_query_1, top_k=100)\n",
    "print(\"\\nQ:\", user_query_1)\n",
    "print(\"A:\", response_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
