{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from common_utils import embed_texts, embed_images, encode_image_to_base64, search_index, retrieve_context, call_gpt_4, extract_figures_from_pdf\n",
    "import faiss\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, pdf_path, openai_api_key):\n",
    "        self.openai_client = OpenAI(api_key=openai_api_key)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model_id = \"openai/clip-vit-base-patch32\"\n",
    "        self.clip_model = CLIPModel.from_pretrained(self.model_id).to(self.device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(self.model_id)\n",
    "        self.pdf_path = pdf_path\n",
    "        self.index = None\n",
    "        self.all_metadata = []\n",
    "        self._process_pdf()\n",
    "        self._build_faiss()\n",
    "    \n",
    "    def _process_pdf(self):\n",
    "        text_data, image_data = [], []\n",
    "        reader = PdfReader(self.pdf_path)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text and page_text.strip():\n",
    "                text_data.append({\"text\": page_text.strip(), \"page_number\": i + 1})\n",
    "        \n",
    "        all_images = extract_figures_from_pdf(self.pdf_path)\n",
    "        for i, pil_img in enumerate(all_images):\n",
    "            image_data.append({\"image\": pil_img, \"image_number\": i + 1})\n",
    "        \n",
    "        texts_list = [td[\"text\"] for td in text_data]\n",
    "        if texts_list:\n",
    "            text_embeddings = embed_texts(texts_list, self.clip_processor, self.clip_model)\n",
    "            for i, emb in enumerate(text_embeddings):\n",
    "                self.all_metadata.append({\"type\": \"text\", \"content\": text_data[i][\"text\"], \"page_number\": text_data[i][\"page_number\"]})\n",
    "        \n",
    "        pil_images_list = [id_[\"image\"] for id_ in image_data]\n",
    "        if pil_images_list:\n",
    "            image_embeddings = embed_images(pil_images_list, self.clip_processor, self.clip_model)\n",
    "            for i, emb in enumerate(image_embeddings):\n",
    "                base64_str = encode_image_to_base64(image_data[i][\"image\"])\n",
    "                self.all_metadata.append({\"type\": \"image\", \"content\": base64_str, \"image_number\": image_data[i][\"image_number\"]})\n",
    "        \n",
    "        self.all_embeddings = np.array(text_embeddings + image_embeddings).astype(\"float32\")\n",
    "        self.embedding_dimension = self.all_embeddings.shape[1]\n",
    "    \n",
    "    def _build_faiss(self):\n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dimension)\n",
    "        self.index.add(self.all_embeddings)\n",
    "    \n",
    "    def get_most_relevant_docs(self, user_query, top_k=3):\n",
    "        query_emb = embed_texts([user_query], self.clip_processor, self.clip_model)\n",
    "        distances, faiss_indices = search_index(self.index, query_emb, top_k=top_k)\n",
    "        return retrieve_context(faiss_indices, self.all_metadata)\n",
    "    \n",
    "    def generate_answer(self, user_query, retrieved_docs):\n",
    "        user_content = [{\"type\": \"text\", \"text\": f\"User query: {user_query}\"}]\n",
    "        for doc in retrieved_docs:\n",
    "            if doc[\"type\"] == \"text\":\n",
    "                user_content.append({\"type\": \"text\", \"text\": f\"(page {doc['page_number']}) {doc['content'][:500]}...\"})\n",
    "            elif doc[\"type\"] == \"image\":\n",
    "                user_content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{doc['content']}\"}})\n",
    "        return call_gpt_4(user_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0 (main, Oct  7 2024, 05:02:14) [Clang 15.0.0 (clang-1500.1.0.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34d35bb7a41995741358303864c0abd463933b7f4684d12ed372b9f8e1810ab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
