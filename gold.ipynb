{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from datasets import load_dataset\n",
    "\n",
    "from document_processor import TextProcessor, ImageProcessor, PageImageProcessor, ImageTextualSummaryProcessor, ImageTextualSummaryProcessorLarge\n",
    "from multimodal_rag import MultimodalRAG\n",
    "from embedder import OpenAIEmbedder, ColPaliEmbedder\n",
    "from pdf_to_qa import generate_qa_for_pdf, generate_chartQA_pdf_and_json\n",
    "from evaluation import evaluate_generation, evaluate_generation_chartQA, compute_mrr_at_k, compute_recall_at_k, compute_precision_at_k, compute_f1_score, compute_map_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first gen of gold answers no batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import call_gpt_4\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "from typing import List\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.metrics import AnswerCorrectness, MultiModalRelevance, MultiModalFaithfulness\n",
    "from ragas import evaluate, EvaluationDataset\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load and format gold dataset\n",
    "def load_gold_dataset(gold_path) -> List[dict]:\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load your dataset (assume it's already a list of dicts)\n",
    "gold_entries = load_gold_dataset(\"QA_coinqa_gold_final.json\")\n",
    "\n",
    "# Process each entry\n",
    "for entry in tqdm(gold_entries, desc=\"Processing entries\"):\n",
    "    base64_str = entry[\"image_base64\"]\n",
    "    question = entry[\"question\"]\n",
    "\n",
    "    user_prompt = [\n",
    "        {\"type\": \"text\", \"text\": question},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_str}\"}}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        generated_answer = call_gpt_4(user_prompt)\n",
    "        entry[\"generated_answer\"] = generated_answer\n",
    "    except Exception as e:\n",
    "        entry[\"generated_answer\"] = f\"Error: {str(e)}\"\n",
    "\n",
    "# Optionally, save back to a file\n",
    "import json\n",
    "with open(\"QA_coinqa_with_generated_answers.json\", \"w\") as f:\n",
    "    json.dump(gold_entries, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second with batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entries: 100%|██████████| 55/55 [23:43<00:00, 25.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from common_utils import call_gpt_4\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load gold dataset\n",
    "def load_gold_dataset(gold_path) -> List[dict]:\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Paths\n",
    "input_path = \"json_files/ChartQA_QA_Mapping.json\"\n",
    "output_path = \"json_files/ChartQA_QA_Mapping_with_generated_answers.json\"\n",
    "\n",
    "\n",
    "gold_entries = load_gold_dataset(input_path)\n",
    "\n",
    "# Load existing results if they exist\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        saved_entries = json.load(f)\n",
    "    completed_questions = {\n",
    "        (e[\"page_number\"], e[\"question\"]) for e in saved_entries if \"generated_answer\" in e\n",
    "    }\n",
    "else:\n",
    "    saved_entries = []\n",
    "    completed_questions = set()\n",
    "\n",
    "# Process and save\n",
    "for i, entry in enumerate(tqdm(gold_entries, desc=\"Processing entries\")):\n",
    "    key = (entry[\"page_number\"], entry[\"question\"])\n",
    "    \n",
    "    if key in completed_questions:\n",
    "        continue  # Skip already processed\n",
    "\n",
    "    base64_str = entry[\"image_base64\"]\n",
    "    question = entry[\"question\"]\n",
    "\n",
    "    user_prompt = [\n",
    "        {\"type\": \"text\", \"text\": question},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_str}\"}}\n",
    "    ]\n",
    "\n",
    "    # Retry loop for rate limits\n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            generated_answer = call_gpt_4(user_prompt)\n",
    "            entry[\"generated_answer\"] = generated_answer\n",
    "            break  # Success, exit retry loop\n",
    "        except Exception as e:\n",
    "            error_message = str(e).lower()\n",
    "            if \"rate limit\" in error_message or \"429\" in error_message:\n",
    "                wait_time = 20 * (attempt + 1)  # Increasing delay on each retry\n",
    "                print(f\"Rate limit hit. Sleeping for {wait_time} seconds (attempt {attempt + 1}/{max_retries})...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                entry[\"generated_answer\"] = f\"Error: {str(e)}\"\n",
    "                break\n",
    "\n",
    "    saved_entries.append(entry)\n",
    "\n",
    "    # Save every 10 entries\n",
    "    if len(saved_entries) % 10 == 0:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(saved_entries, f, indent=2)\n",
    "\n",
    "    # Fixed delay after each entry to prevent hitting rate limits\n",
    "    time.sleep(20)\n",
    "\n",
    "# Final save\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(saved_entries, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_entries = load_gold_dataset(\"QA_coinqa_with_generated_answers.json\")\n",
    "for item in gold_entries:\n",
    "    print(f\"Generated Answer: {item['generated_answer']}\")\n",
    "    print(f\"Reference Answer: {item['answer']}\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43d69d707c445af8c9e6287c0b977de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for batch 1 saved to: resultsbatch/batch_1.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c2b7bdbff2479080a4f9aaeac76545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for batch 2 saved to: resultsbatch/batch_2.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55713afe36974506843564ccd170513e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for batch 3 saved to: resultsbatch/batch_3.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32de0755d91e43bd8859511158d90595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for batch 4 saved to: resultsbatch/batch_4.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead8042d24374619bd2db2ab0503174f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for batch 5 saved to: resultsbatch/batch_5.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c5f876451240d5a09ba9d6aeb0fac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for batch 6 saved to: resultsbatch/batch_6.json\n",
      "Final average metrics: {'average_faithful_rate': 0.0, 'average_relevance_rate': 0.0, 'average_answer_correctness': 0.5320666666666667}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from typing import List\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.metrics import AnswerCorrectness, MultiModalRelevance, MultiModalFaithfulness\n",
    "from ragas import evaluate, EvaluationDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\")) # For Ragas evaluation\n",
    "\n",
    "# Load and format gold dataset\n",
    "def load_gold_dataset(gold_path) -> List[dict]:\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "# Format entries for Ragas\n",
    "def format_for_ragas(gold_entries: List[dict]) -> List[dict]:\n",
    "    formatted = []\n",
    "    for item in gold_entries:\n",
    "        formatted.append({\n",
    "            \"user_input\": item[\"question\"],\n",
    "            #\"retrieved_contexts\": [item[\"image_base64\"]],\n",
    "            \"response\": item[\"generated_answer\"],\n",
    "            \"reference\": item[\"answer\"]\n",
    "        })\n",
    "    return formatted\n",
    "\n",
    "# Define the file path for storing the results\n",
    "results_folder = \"resultsbatch\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "def evaluate_generation_chartQA(rag_answers: List[dict], evaluator_llm):\n",
    "    evaluation_dataset = EvaluationDataset.from_list(rag_answers)\n",
    "    result = evaluate(\n",
    "        dataset=evaluation_dataset, \n",
    "        metrics=[AnswerCorrectness()], \n",
    "        llm=evaluator_llm,\n",
    "    )\n",
    "    return result  # Returning raw result for debugging\n",
    "\n",
    "# Function to evaluate in batches\n",
    "def evaluate_in_batches(rag_generated_answers, evaluator_llm, batch_size=10):\n",
    "    total_metrics = {\"faithful_rate\": 0.0, \"relevance_rate\": 0.0, \"answer_correctness\": 0.0}\n",
    "    total_batches = len(rag_generated_answers) // batch_size + (1 if len(rag_generated_answers) % batch_size > 0 else 0)\n",
    "    \n",
    "    for batch_index in range(total_batches):\n",
    "        # Define the batch's results file path\n",
    "        batch_file_path = os.path.join(results_folder, f\"batch_{batch_index+1}.json\")\n",
    "        \n",
    "        # Check if the batch result already exists\n",
    "        if os.path.exists(batch_file_path):\n",
    "            print(f\"Batch {batch_index+1} results already exist: {batch_file_path}\")\n",
    "            with open(batch_file_path, 'r', encoding='utf-8') as file:\n",
    "                batch_results = json.load(file)\n",
    "                print(f\"Loaded batch {batch_index+1} results: {batch_results}\")\n",
    "        else:\n",
    "            # Create a subset of the answers for this batch\n",
    "            batch_start = batch_index * batch_size\n",
    "            batch_end = min((batch_index + 1) * batch_size, len(rag_generated_answers))\n",
    "            subset_rag_generated_answers = rag_generated_answers[batch_start:batch_end]\n",
    "            \n",
    "            # Evaluate the batch\n",
    "            faithfulness_and_relevance = str(evaluate_generation_chartQA(subset_rag_generated_answers, evaluator_llm))\n",
    "            \n",
    "            # Replace single quotes with double quotes\n",
    "            json_string = faithfulness_and_relevance.replace(\"'\", '\"')\n",
    "            \n",
    "            # Save the batch results\n",
    "            with open(batch_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(json_string)  # Write string instead of using json.dump()\n",
    "            print(f\"Results for batch {batch_index+1} saved to: {batch_file_path}\")\n",
    "            \n",
    "            # Load the batch results into the dictionary\n",
    "            batch_results = json.loads(json_string)\n",
    "        \n",
    "        # Accumulate metrics for averaging\n",
    "        total_metrics[\"faithful_rate\"] += batch_results.get(\"faithful_rate\", 0)\n",
    "        total_metrics[\"relevance_rate\"] += batch_results.get(\"relevance_rate\", 0)\n",
    "        total_metrics[\"answer_correctness\"] += batch_results.get(\"answer_correctness\", 0)\n",
    "    \n",
    "    # Calculate the average of the metrics\n",
    "    total_batches = max(1, total_batches)  # Avoid division by zero in case of empty input\n",
    "    average_metrics = {\n",
    "        \"average_faithful_rate\": total_metrics[\"faithful_rate\"] / total_batches,\n",
    "        \"average_relevance_rate\": total_metrics[\"relevance_rate\"] / total_batches,\n",
    "        \"average_answer_correctness\": total_metrics[\"answer_correctness\"] / total_batches\n",
    "    }\n",
    "\n",
    "    return average_metrics\n",
    "\n",
    "# Example usage of the function\n",
    "if __name__ == \"__main__\":\n",
    "    gold_entries = load_gold_dataset(\"QA_coinqa_with_generated_answers.json\")\n",
    "    #gold_entries = load_gold_dataset(\"json_files/ChartQA_QA_Mapping.json\")\n",
    "\n",
    "    # Format for evaluation\n",
    "    formatted = format_for_ragas(gold_entries)\n",
    "\n",
    "    # Assuming rag_generated_answers and evaluator_llm are already defined\n",
    "    average_metrics = evaluate_in_batches(formatted, evaluator_llm)\n",
    "    print(\"Final average metrics:\", average_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dont know what the code is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import time\n",
    "from typing import List\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.metrics import AnswerCorrectness, MultiModalRelevance, MultiModalFaithfulness\n",
    "from ragas import evaluate, EvaluationDataset\n",
    "from datasets import Dataset\n",
    "from openai import RateLimitError\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize LLM wrapper\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "\n",
    "# Load and format gold dataset\n",
    "def load_gold_dataset(gold_path) -> List[dict]:\n",
    "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Format entries for Ragas\n",
    "def format_for_ragas(gold_entries: List[dict]) -> List[dict]:\n",
    "    formatted = []\n",
    "    for item in gold_entries:\n",
    "        formatted.append({\n",
    "            \"user_input\": item[\"question\"],\n",
    "            \"retrieved_contexts\": [item[\"image_base64\"]],\n",
    "            \"response\": item[\"answer\"],\n",
    "            \"reference\": item[\"answer\"]\n",
    "        })\n",
    "    return formatted\n",
    "\n",
    "# Define the file path for storing the results\n",
    "results_folder = \"resultsbatch\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_generation_chartQA(rag_answers: List[dict], evaluator_llm):\n",
    "    evaluation_dataset = EvaluationDataset.from_list(rag_answers)\n",
    "    result = evaluate(\n",
    "        dataset=evaluation_dataset,\n",
    "        metrics=[MultiModalFaithfulness(), MultiModalRelevance()],\n",
    "        llm=evaluator_llm,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Batch evaluation function with retry and sleep\n",
    "def evaluate_in_batches(rag_generated_answers, evaluator_llm, batch_size=50):\n",
    "    total_metrics = {\"faithful_rate\": 0.0, \"relevance_rate\": 0.0, \"answer_correctness\": 0.0}\n",
    "    total_batches = len(rag_generated_answers) // batch_size + (1 if len(rag_generated_answers) % batch_size > 0 else 0)\n",
    "\n",
    "    for batch_index in range(total_batches):\n",
    "        batch_file_path = os.path.join(results_folder, f\"batch_{batch_index+1}.json\")\n",
    "\n",
    "        if os.path.exists(batch_file_path):\n",
    "            print(f\"Batch {batch_index+1} results already exist: {batch_file_path}\")\n",
    "            with open(batch_file_path, 'r', encoding='utf-8') as file:\n",
    "                batch_results = json.load(file)\n",
    "                print(f\"Loaded batch {batch_index+1} results: {batch_results}\")\n",
    "        else:\n",
    "            batch_start = batch_index * batch_size\n",
    "            batch_end = min((batch_index + 1) * batch_size, len(rag_generated_answers))\n",
    "            subset_rag_generated_answers = rag_generated_answers[batch_start:batch_end]\n",
    "\n",
    "            # Retry logic for rate limiting\n",
    "            max_retries = 5\n",
    "            retry_delay = 2  # seconds\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    faithfulness_and_relevance = str(evaluate_generation_chartQA(subset_rag_generated_answers, evaluator_llm))\n",
    "                    break\n",
    "                except RateLimitError as e:\n",
    "                    print(f\"Rate limit hit on batch {batch_index+1} (attempt {attempt+1}/{max_retries}). Retrying in {retry_delay}s...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2\n",
    "            else:\n",
    "                raise RuntimeError(f\"Failed to evaluate batch {batch_index+1} after {max_retries} retries due to rate limits.\")\n",
    "\n",
    "            # Convert to JSON string and save\n",
    "            json_string = faithfulness_and_relevance.replace(\"'\", '\"')\n",
    "            with open(batch_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(json_string)\n",
    "            print(f\"Results for batch {batch_index+1} saved to: {batch_file_path}\")\n",
    "\n",
    "            batch_results = json.loads(json_string)\n",
    "\n",
    "            # Sleep between batches to prevent hitting limits\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Accumulate metrics\n",
    "        total_metrics[\"faithful_rate\"] += batch_results.get(\"faithful_rate\", 0)\n",
    "        total_metrics[\"relevance_rate\"] += batch_results.get(\"relevance_rate\", 0)\n",
    "        total_metrics[\"answer_correctness\"] += batch_results.get(\"answer_correctness\", 0)\n",
    "\n",
    "    # Compute averages\n",
    "    total_batches = max(1, total_batches)\n",
    "    average_metrics = {\n",
    "        \"average_faithful_rate\": total_metrics[\"faithful_rate\"] / total_batches,\n",
    "        \"average_relevance_rate\": total_metrics[\"relevance_rate\"] / total_batches,\n",
    "        \"average_answer_correctness\": total_metrics[\"answer_correctness\"] / total_batches\n",
    "    }\n",
    "\n",
    "    return average_metrics\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     gold_entries = load_gold_dataset(\"json_files/ChartQA_QA_Mapping.json\")\n",
    "#     formatted = format_for_ragas(gold_entries)\n",
    "#     average_metrics = evaluate_in_batches(formatted, evaluator_llm)\n",
    "#     print(\"Final average metrics:\", average_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def calculate_average_metrics(results_folder):\n",
    "    total_metrics = {\"faithful_rate\": 0.0, \"relevance_rate\": 0.0, \"answer_correctness\": 0.0}\n",
    "    file_count = 0\n",
    "\n",
    "    for filename in os.listdir(results_folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(results_folder, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                metrics = json.load(f)\n",
    "                total_metrics[\"faithful_rate\"] += metrics.get(\"faithful_rate\", 0)\n",
    "                total_metrics[\"relevance_rate\"] += metrics.get(\"relevance_rate\", 0)\n",
    "                total_metrics[\"answer_correctness\"] += metrics.get(\"answer_correctness\", 0)\n",
    "                file_count += 1\n",
    "\n",
    "    if file_count == 0:\n",
    "        print(\"No result files found in the folder.\")\n",
    "        return None\n",
    "\n",
    "    average_metrics = {\n",
    "        \"average_faithful_rate\": total_metrics[\"faithful_rate\"] / file_count,\n",
    "        \"average_relevance_rate\": total_metrics[\"relevance_rate\"] / file_count,\n",
    "        \"average_answer_correctness\": total_metrics[\"answer_correctness\"] / file_count\n",
    "    }\n",
    "\n",
    "    return average_metrics\n",
    "\n",
    "# Calculate and print the average metrics\n",
    "average_metrics = calculate_average_metrics(results_folder)\n",
    "print(\"Average Metrics:\", average_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "\n",
    "def generate_chartQA_json_batched(dataset, output_path='json_files/ChartQA_QA_Mapping.json', batch_size=50, max_entries=200):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    qa_list = []\n",
    "    current_page = 0\n",
    "    processed_count = 0\n",
    "    batch_progress = 1\n",
    "\n",
    "    for data in tqdm(dataset, desc=\"Processing Charts\", unit=\"chart\"):\n",
    "        if data.get('type') != 'human_test':\n",
    "            continue\n",
    "\n",
    "        image = data['image']\n",
    "\n",
    "        # Convert image to bytes and encode as base64\n",
    "        image_bytes = io.BytesIO()\n",
    "        image.save(image_bytes, format=\"PNG\")\n",
    "        image_bytes = image_bytes.getvalue()\n",
    "        image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "        current_page += 1\n",
    "\n",
    "        qa_list.append({\n",
    "            'page_number': current_page,\n",
    "            'question': data['question'],\n",
    "            'answer': data['answer'],\n",
    "            'image_base64': image_base64,\n",
    "            'type': data['type']\n",
    "        })\n",
    "\n",
    "        processed_count += 1\n",
    "\n",
    "        if processed_count % batch_size == 0:\n",
    "            print(f'📦 Processed batch {batch_progress} ({processed_count} entries so far)')\n",
    "            batch_progress += 1\n",
    "\n",
    "        if processed_count >= max_entries:\n",
    "            break\n",
    "\n",
    "    # Save all results to one JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(qa_list, f, indent=4)\n",
    "\n",
    "    print(f'✅ Done! Saved {len(qa_list)} entries to: {output_path}')\n",
    "\n",
    "\n",
    "\n",
    "# Example usage (assuming the dataset is already loaded)\n",
    "from datasets import load_dataset\n",
    "chartqa = load_dataset('lmms-lab/ChartQA', split='test')\n",
    "generate_chartQA_json_batched(chartqa, output_path='json_files/ChartQA_QA_Mapping.json', batch_size=50, max_entries=1300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "def format_for_ragas(gold_entries: List[dict]) -> List[dict]:\n",
    "    formatted = []\n",
    "    for item in gold_entries:\n",
    "        formatted.append({\n",
    "            \"user_input\": item[\"question\"],\n",
    "            \"retrieved_contexts\": [item[\"image_base64\"]],\n",
    "            \"response\": item[\"answer\"],\n",
    "            \"reference\": item[\"answer\"]\n",
    "        })\n",
    "    return formatted\n",
    "\n",
    "# Resize base64 image to a reasonable width (e.g. 400px)\n",
    "def resize_base64_image(base64_str: str, target_width: int = 400) -> str:\n",
    "    image_data = base64.b64decode(base64_str)\n",
    "    with PILImage.open(io.BytesIO(image_data)) as img:\n",
    "        # Maintain aspect ratio\n",
    "        ratio = target_width / float(img.width)\n",
    "        target_height = int(img.height * ratio)\n",
    "        resized_img = img.resize((target_width, target_height), PILImage.Resampling.LANCZOS)\n",
    "\n",
    "        # Convert back to base64\n",
    "        buffer = io.BytesIO()\n",
    "        resized_img.save(buffer, format=\"PNG\")\n",
    "        return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "# Display and evaluate selected images\n",
    "# def evaluate_selected_images(gold_path: str, image_indices: List[int], evaluator_llm):\n",
    "#     gold_entries = load_gold_dataset(gold_path)\n",
    "    \n",
    "#     # Check if indices are valid\n",
    "#     max_index = len(gold_entries) - 1\n",
    "#     for idx in image_indices:\n",
    "#         if idx > max_index or idx < 0:\n",
    "#             raise IndexError(f\"Index {idx} is out of range. Dataset has {len(gold_entries)} entries.\")\n",
    "        \n",
    "#     selected_entries = [gold_entries[i] for i in image_indices]\n",
    "\n",
    "#     # Resize images before formatting\n",
    "#     for entry in selected_entries:\n",
    "#         if \"image_base64\" in entry:\n",
    "#             entry[\"image_base64\"] = resize_base64_image(entry[\"image_base64\"])\n",
    "\n",
    "#     formatted_entries = format_for_ragas(selected_entries)\n",
    "\n",
    "#     # Print all retrieved context images\n",
    "#     for entry in formatted_entries:\n",
    "#         print(\"Context Images:\")\n",
    "#         for idx, context in enumerate(entry[\"retrieved_contexts\"]):\n",
    "#             print(f\"Image {idx + 1}:\")\n",
    "#             print(f\"Quetsion : {entry['user_input']}\")\n",
    "#             print(f\"Answer : {entry['response']}\")\n",
    "#             display(PILImage.open(io.BytesIO(base64.b64decode(context))))\n",
    "\n",
    "#     # Evaluate\n",
    "#     results = evaluate_generation_chartQA(formatted_entries, evaluator_llm)\n",
    "#     print(\"\\nEvaluation Metrics:\")\n",
    "#     print(results)\n",
    "#     return results\n",
    "\n",
    "def evaluate_selected_images(gold_path: str, image_indices: List[int], evaluator_llm):\n",
    "    gold_entries = load_gold_dataset(gold_path)\n",
    "\n",
    "    # Check if indices are valid\n",
    "    max_index = len(gold_entries) - 1\n",
    "    for idx in image_indices:\n",
    "        if idx > max_index or idx < 0:\n",
    "            raise IndexError(f\"Index {idx} is out of range. Dataset has {len(gold_entries)} entries.\")\n",
    "\n",
    "    # Process each image independently\n",
    "    for i in image_indices:\n",
    "        entry = gold_entries[i]\n",
    "\n",
    "        # Resize image\n",
    "        if \"image_base64\" in entry:\n",
    "            entry[\"image_base64\"] = resize_base64_image(entry[\"image_base64\"])\n",
    "\n",
    "        # Format entry for evaluation\n",
    "        formatted_entry = format_for_ragas([entry])\n",
    "\n",
    "        # Display context image and related info\n",
    "        print(f\"\\n--- Evaluation for Entry {i} ---\")\n",
    "        print(\"Context Image:\")\n",
    "        print(f\"Question: {formatted_entry[0]['user_input']}\")\n",
    "        print(f\"Answer: {formatted_entry[0]['response']}\")\n",
    "        display(PILImage.open(io.BytesIO(base64.b64decode(formatted_entry[0]['retrieved_contexts'][0]))))\n",
    "\n",
    "        # Evaluate this entry\n",
    "        result = evaluate_generation_chartQA(formatted_entry, evaluator_llm)\n",
    "\n",
    "        # Print evaluation result\n",
    "        print(\"Evaluation Metrics:\")\n",
    "        print(result)\n",
    "\n",
    "\n",
    "# Example: Evaluate image 3 and 7 from the dataset\n",
    "#results = evaluate_selected_images(\"json_files/ChartQA_QA_Mapping.json\", [1,2], evaluator_llm)\n",
    "\n",
    "with open(\"QA_coinqa_gold_final.json\", 'r', encoding='utf-8') as f:\n",
    "    qa_data = json.load(f)\n",
    "#image_indices = list(range(1,)))\n",
    "\n",
    "# Process the first third of the images\n",
    "#image_indices = list(range(len(qa_data) // 3))\n",
    "\n",
    "# Code for the second third of the images (commented out)\n",
    "#image_indices = list(range(len(qa_data) // 3, 2 * len(qa_data) // 3))\n",
    "\n",
    "# Code for the last third of the images (commented out)\n",
    "#image_indices = list(range(2 * len(qa_data) // 3, len(qa_data)))\n",
    "\n",
    "image_indices = list(range(10, 21))\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\")) # For Ragas evaluation\n",
    "results = evaluate_selected_images(\"QA_coinqa_gold_final.json\", list(range(52, 54)), evaluator_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from typing import List\n",
    "from json import JSONDecodeError\n",
    "\n",
    "def evaluate_selected_images(gold_path: str, image_indices: List[int], evaluator_llm, batch_size: int = 10):\n",
    "    gold_entries = load_gold_dataset(gold_path)\n",
    "    zero_entries = []\n",
    "    faithfulness_zeros = 0\n",
    "    relevance_zeros = 0\n",
    "    total_entries = 0\n",
    "    skipped_entries = []\n",
    "\n",
    "    # Check if indices are valid\n",
    "    max_index = len(gold_entries) - 1\n",
    "    for idx in image_indices:\n",
    "        if idx > max_index or idx < 0:\n",
    "            raise IndexError(f\"Index {idx} is out of range. Dataset has {len(gold_entries)} entries.\")\n",
    "\n",
    "    # Process in batches, but evaluate individually\n",
    "    for batch_start in range(0, len(image_indices), batch_size):\n",
    "        batch_indices = image_indices[batch_start:batch_start + batch_size]\n",
    "\n",
    "        for i in batch_indices:\n",
    "            entry = gold_entries[i]\n",
    "            total_entries += 1\n",
    "\n",
    "            try:\n",
    "                # Resize image if exists\n",
    "                if \"image_base64\" in entry:\n",
    "                    entry[\"image_base64\"] = resize_base64_image(entry[\"image_base64\"])\n",
    "\n",
    "                # Format and evaluate this single entry\n",
    "                formatted_entry = format_for_ragas([entry])\n",
    "                raw_result = str(evaluate_generation_chartQA(formatted_entry, evaluator_llm))\n",
    "                json_string = raw_result.replace(\"'\", '\"')\n",
    "                result = json.loads(json_string)\n",
    "\n",
    "                # Extract metrics\n",
    "                faithfulness_score = result.get(\"faithful_rate\")\n",
    "                relevance_score = result.get(\"relevance_rate\")\n",
    "\n",
    "                if faithfulness_score == 0 or relevance_score == 0:\n",
    "                    zero_entries.append({\n",
    "                        \"entry_index\": i,\n",
    "                        \"faithfulness\": faithfulness_score,\n",
    "                        \"relevance\": relevance_score\n",
    "                    })\n",
    "\n",
    "                if faithfulness_score == 0:\n",
    "                    faithfulness_zeros += 1\n",
    "                if relevance_score == 0:\n",
    "                    relevance_zeros += 1\n",
    "\n",
    "            except (JSONDecodeError, KeyError) as e:\n",
    "                print(f\"⚠️ JSON decode or key error at entry {i}: {e}\")\n",
    "                skipped_entries.append(i)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing entry {i}: {e}\")\n",
    "                skipped_entries.append(i)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\nEntries with 0 in either Faithfulness or Relevance:\")\n",
    "    for ze in zero_entries:\n",
    "        print(f\"Entry {ze['entry_index']}: Faithfulness = {ze['faithfulness']}, Relevance = {ze['relevance']}\")\n",
    "\n",
    "    print(\"\\nShare of 0 scores:\")\n",
    "    print(f\"Faithfulness: {faithfulness_zeros}/{total_entries} ({(faithfulness_zeros / total_entries * 100):.2f}%)\")\n",
    "    print(f\"Relevance: {relevance_zeros}/{total_entries} ({(relevance_zeros / total_entries * 100):.2f}%)\")\n",
    "\n",
    "    if skipped_entries:\n",
    "        print(\"\\n⚠️ Skipped entries due to errors:\", skipped_entries)\n",
    "\n",
    "\n",
    "# Load QA data and choose indices\n",
    "with open(\"json_files/ChartQA_QA_Mapping.json\", 'r', encoding='utf-8') as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "image_indices = random.sample(range(len(qa_data)), 50)\n",
    "\n",
    "# Evaluation\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "evaluate_selected_images(\"json_files/ChartQA_QA_Mapping.json\", image_indices, evaluator_llm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
