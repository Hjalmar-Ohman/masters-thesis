{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from datasets import load_dataset\n",
    "\n",
    "from document_processor import TextProcessor, ImageProcessor, PageImageProcessor, ImageTextualSummaryProcessor, ImageTextualSummaryProcessorLarge\n",
    "from multimodal_rag import MultimodalRAG\n",
    "from embedder import OpenAIEmbedder, ColPaliEmbedder\n",
    "from pdf_to_qa import generate_qa_for_pdf, generate_chartQA_pdf_and_json\n",
    "from evaluation import evaluate_generation, evaluate_generation_chartQA, compute_mrr_at_k, compute_recall_at_k, compute_precision_at_k, compute_f1_score, compute_map_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"CoinQA\"\n",
    "k = 10\n",
    "\n",
    "if dataset.upper() == \"COINQA\":\n",
    "    #PDF_FILE = \"knowledge/subset_riksbanken.pdf\"\n",
    "    PDF_FILE = \"knowledge/riksbanken.pdf\"\n",
    "elif dataset.upper() == \"CHARTQA\":\n",
    "    #PDF_FILE = \"knowledge/subset_ChartQA.pdf\"\n",
    "    PDF_FILE = \"knowledge/ChartQA_human.pdf\"\n",
    "else:\n",
    "    raise ValueError(\"Dataset not supported\")\n",
    "\n",
    "#text_processor = TextProcessor(OpenAIEmbedder())\n",
    "#image_processor = ImageProcessor(ColPaliEmbedder(), dataset)\n",
    "#page_image_processor = PageImageProcessor(ColPaliEmbedder())\n",
    "image_textual_summary_processor = ImageTextualSummaryProcessor(OpenAIEmbedder(), dataset)\n",
    "\n",
    "rag = MultimodalRAG([image_textual_summary_processor], PDF_FILE)\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\")) # For Ragas evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "def merge_batches(batch_dir=\"embedding_cache/batches\", output_dir=\"embedding_cache\", name=\"ImageTextualSummaryProcessor\"):\n",
    "    all_embeddings = []\n",
    "    all_metadata = []\n",
    "\n",
    "    batch_files = sorted(os.listdir(batch_dir))\n",
    "    embedding_files = [f for f in batch_files if f.endswith(\"_embeddings.pt\")]\n",
    "\n",
    "    for embed_file in embedding_files:\n",
    "        base = embed_file.replace(\"_embeddings.pt\", \"\")\n",
    "        meta_file = f\"{base}_metadata.json\"\n",
    "\n",
    "        emb = torch.load(os.path.join(batch_dir, embed_file))\n",
    "\n",
    "        if isinstance(emb, list):\n",
    "            for item in emb:\n",
    "                if isinstance(item, torch.Tensor):\n",
    "                    all_embeddings.append(item)\n",
    "                elif isinstance(item, list):\n",
    "                    if all(isinstance(x, float) for x in item):  # plain embedding\n",
    "                        all_embeddings.append(torch.tensor(item))\n",
    "                    elif all(isinstance(x, torch.Tensor) for x in item):  # list of tensors\n",
    "                        all_embeddings.extend(item)\n",
    "                    else:\n",
    "                        raise TypeError(f\"Unexpected item list contents: {item}\")\n",
    "                else:\n",
    "                    raise TypeError(f\"Unexpected embedding item: {type(item)}\")\n",
    "        elif isinstance(emb, torch.Tensor):\n",
    "            all_embeddings.append(emb)\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected embedding type: {type(emb)}\")\n",
    "\n",
    "        # Load metadata\n",
    "        with open(os.path.join(batch_dir, meta_file)) as f:\n",
    "            meta = json.load(f)\n",
    "        all_metadata.extend(meta)\n",
    "\n",
    "    merged_embeddings = torch.stack(all_embeddings, dim=0)\n",
    "\n",
    "    torch.save(merged_embeddings, os.path.join(output_dir, f\"{name}_merged_embeddings.pt\"))\n",
    "    with open(os.path.join(output_dir, f\"{name}_merged_metadata.json\"), \"w\") as f:\n",
    "        json.dump(all_metadata, f)\n",
    "\n",
    "    print(f\"Merged {len(embedding_files)} batches into {output_dir} with {len(all_embeddings)} embeddings.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_batches()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if QA file already exists\n",
    "qa_filepath = \"json_files/QA_\" + os.path.basename(PDF_FILE).replace('.pdf', '.json')\n",
    "\n",
    "if os.path.exists(qa_filepath):\n",
    "    print(f\"Using existing QA file: {qa_filepath}\")\n",
    "\n",
    "elif dataset.upper() == \"COINQA\":\n",
    "    generate_qa_for_pdf(PDF_FILE, json_output_path=qa_filepath, mode=\"per_image\")\n",
    "    print(f\"Generated new CoinQA file: {qa_filepath}\")\n",
    "\n",
    "elif dataset.upper() == \"CHARTQA\":\n",
    "    #chartqa = load_dataset('lmms-lab/ChartQA', split='test')\n",
    "    #subset_chartqa = chartqa.select(range(20))\n",
    "    \n",
    "    generate_chartQA_pdf_and_json(subset_chartqa, pdf_output_path=PDF_FILE, json_output_path=qa_filepath)\n",
    "    print(f\"Generated new ChartQA file: {qa_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Answering the QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(qa_filepath, 'r', encoding='utf-8') as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "# Generate dataset\n",
    "rag_generated_answers = []\n",
    "\n",
    "# Check if generated answers file already exists\n",
    "rag_answers_path = \"json_files/rag_generated_answers_\" + os.path.basename(qa_filepath).replace('.json', f'_{rag.name}.json')\n",
    "\n",
    "if os.path.exists(rag_answers_path):\n",
    "    with open(rag_answers_path, 'r', encoding='utf-8') as f:\n",
    "        rag_generated_answers = json.load(f)\n",
    "    start_index = len(rag_generated_answers)\n",
    "    print(f\"Using existing RAG generated answers file: {rag_answers_path}\")\n",
    "    print(f\"Resuming from question {start_index + 1}\")\n",
    "else:\n",
    "    start_index = 0\n",
    "\n",
    "batch_size = 20\n",
    "total_questions = len(qa_data)\n",
    "output_dataset_file = rag_answers_path\n",
    "\n",
    "for i in range(start_index, total_questions):\n",
    "    qa = qa_data[i]\n",
    "    query = qa[\"question\"]\n",
    "    reference = qa[\"answer\"]\n",
    "    \n",
    "    # Get relevant documents and generate answer\n",
    "    relevant_docs = rag.get_most_relevant_docs(query, top_k=k)\n",
    "    #response = rag.generate_answer_chartQA(query, relevant_docs)\n",
    "\n",
    "    rag_generated_answers.append(\n",
    "        {\n",
    "            \"query\": query,\n",
    "            \"retrieved_contexts\": relevant_docs,\n",
    "            #\"generated_answer\": response,\n",
    "            \"true_answer\": reference\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save intermediate results every batch_size questions\n",
    "    if (i + 1) % batch_size == 0 or (i + 1) == total_questions:\n",
    "        # Remove 'content' from retrieved_contexts before saving\n",
    "        for item in rag_generated_answers:\n",
    "            for idx, context in enumerate(item[\"retrieved_contexts\"]):\n",
    "                if idx > 0:\n",
    "                    context.pop(\"content\", None)\n",
    "\n",
    "        with open(output_dataset_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(rag_generated_answers, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Saved progress: {i + 1}/{total_questions} questions processed.\")\n",
    "\n",
    "print(f\"Generated new RAG generated answers file: {output_dataset_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Evaluate retrieval right away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_generated_answers = []\n",
    "\n",
    "# Check if generated answers file already exists\n",
    "rag_answers_path = \"json_files/rag_generated_answers_\" + os.path.basename(qa_filepath).replace('.json', f'_{rag.name}.json')\n",
    "\n",
    "if os.path.exists(rag_answers_path):\n",
    "    with open(rag_answers_path, 'r', encoding='utf-8') as f:\n",
    "        rag_generated_answers = json.load(f)\n",
    "\n",
    "with open(qa_filepath, 'r', encoding='utf-8') as f:\n",
    "    qa_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Tuple\n",
    "\n",
    "def normalize_input(\n",
    "    retrieved: Union[List[List[str]], List[str]], \n",
    "    true_pages: Union[List[List[str]], List[str]]  # was `real`\n",
    ") -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    if isinstance(retrieved[0], str):  # Single query\n",
    "        return [list(map(str, retrieved))], [list(map(str, true_pages))]\n",
    "    else:\n",
    "        return (\n",
    "            [list(map(str, r)) for r in retrieved],\n",
    "            [list(map(str, r)) for r in true_pages]\n",
    "        )\n",
    "\n",
    "\n",
    "def compute_mrr_at_k(\n",
    "    all_retrieved_pages: Union[List[List[str]], List[str]], \n",
    "    all_real_pages: Union[List[List[str]], List[str]], \n",
    "    k: int\n",
    ") -> float:\n",
    "    all_retrieved_pages, all_real_pages = normalize_input(all_retrieved_pages, all_real_pages)\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for retrieved_pages, real_pages in zip(all_retrieved_pages, all_real_pages):\n",
    "        top_k_pages = retrieved_pages[:k]\n",
    "        rank = float('inf')\n",
    "\n",
    "        for real_page in real_pages:\n",
    "            if real_page in top_k_pages:\n",
    "                rank = min(rank, top_k_pages.index(real_page) + 1)  # 1-based\n",
    "\n",
    "        reciprocal_ranks.append(1.0 / rank if rank != float('inf') else 0.0)\n",
    "\n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "\n",
    "def compute_recall_at_k(\n",
    "    all_retrieved_pages: Union[List[List[str]], List[str]], \n",
    "    all_real_pages: Union[List[List[str]], List[str]], \n",
    "    k: int\n",
    ") -> float:\n",
    "    all_retrieved_pages, all_real_pages = normalize_input(all_retrieved_pages, all_real_pages)\n",
    "    total_hits = 0\n",
    "    total_relevant = 0\n",
    "\n",
    "    for retrieved_pages, real_pages in zip(all_retrieved_pages, all_real_pages): \n",
    "        top_k_pages = retrieved_pages[:k]\n",
    "        hits = sum(1 for page in real_pages if page in top_k_pages)\n",
    "        total_hits += hits\n",
    "        total_relevant += len(real_pages)\n",
    "\n",
    "    return total_hits / total_relevant if total_relevant else 0.0\n",
    "\n",
    "\n",
    "def compute_precision_at_k(\n",
    "    all_retrieved_pages: Union[List[List[str]], List[str]], \n",
    "    all_real_pages: Union[List[List[str]], List[str]], \n",
    "    k: int\n",
    ") -> float:\n",
    "    all_retrieved_pages, all_real_pages = normalize_input(all_retrieved_pages, all_real_pages)\n",
    "    total_hits = 0\n",
    "    total_retrieved = 0\n",
    "\n",
    "    for retrieved_pages, real_pages in zip(all_retrieved_pages, all_real_pages):\n",
    "        top_k_pages = retrieved_pages[:k]\n",
    "        hits = sum(1 for page in top_k_pages if page in real_pages)\n",
    "        total_hits += hits\n",
    "        total_retrieved += len(top_k_pages)\n",
    "\n",
    "    return total_hits / total_retrieved if total_retrieved else 0.0\n",
    "\n",
    "\n",
    "def compute_f1_score(\n",
    "    all_retrieved_pages: Union[List[List[str]], List[str]], \n",
    "    all_real_pages: Union[List[List[str]], List[str]], \n",
    "    k: int\n",
    ") -> float:\n",
    "    precision = compute_precision_at_k(all_retrieved_pages, all_real_pages, k)\n",
    "    recall = compute_recall_at_k(all_retrieved_pages, all_real_pages, k)\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_map_at_k(\n",
    "    all_retrieved_pages: Union[List[List[str]], List[str]], \n",
    "    all_real_pages: Union[List[List[str]], List[str]], \n",
    "    k: int\n",
    ") -> float:\n",
    "    all_retrieved_pages, all_real_pages = normalize_input(all_retrieved_pages, all_real_pages)\n",
    "    average_precisions = []\n",
    "\n",
    "    for retrieved_pages, real_pages in zip(all_retrieved_pages, all_real_pages):\n",
    "        top_k_pages = retrieved_pages[:k]\n",
    "        num_relevant = 0\n",
    "        precision_sum = 0.0\n",
    "\n",
    "        for i, page in enumerate(top_k_pages):\n",
    "            if page in real_pages:\n",
    "                num_relevant += 1\n",
    "                precision_sum += num_relevant / (i + 1)  # 1-based index\n",
    "\n",
    "        average_precisions.append(precision_sum / num_relevant if num_relevant > 0 else 0.0)\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions) if average_precisions else 0.0\n",
    "\n",
    "\n",
    "all_real_pages, all_retrieved_pages = [], []\n",
    "\n",
    "for rag_answer in rag_generated_answers:\n",
    "    real_page = next(\n",
    "        item[\"page_number\"] for item in qa_data if item[\"question\"] == rag_answer[\"query\"]\n",
    "    )\n",
    "    \n",
    "    #retrieved_pages = [doc[\"page_number\"] for doc in rag_answer[\"retrieved_contexts\"]]\n",
    "    retrieved_pages = [str(doc[\"page_number\"]) for doc in rag_answer[\"retrieved_contexts\"]]\n",
    "\n",
    "    all_real_pages.append([str(real_page)] if isinstance(real_page, (int, float)) else [str(p) for p in real_page])\n",
    "\n",
    "    #all_real_pages.append([real_page] if isinstance(real_page, int) else real_page)\n",
    "    all_retrieved_pages.append(retrieved_pages)\n",
    "\n",
    "# Function to test a specific question by index\n",
    "def test_question(index):\n",
    "    if index < 1 or index > len(rag_generated_answers):\n",
    "        print(\"Invalid index. Please select a number between 1 and 5.\")\n",
    "        return\n",
    "\n",
    "    rag_answer = rag_generated_answers[index - 1]\n",
    "    real_page = all_real_pages[index - 1]\n",
    "    retrieved_pages = all_retrieved_pages[index - 1]\n",
    "\n",
    "    print(f\"Question: {rag_answer['query']}\")\n",
    "    print(f\"True Answer: {rag_answer['true_answer']}\")\n",
    "    #print(f\"Generated Answer: {rag_answer['generated_answer']}\")\n",
    "    print(f\"Real Page(s): {real_page}\")\n",
    "    print(f\"Retrieved Pages: {retrieved_pages}\")\n",
    "    return real_page, retrieved_pages\n",
    "\n",
    "#Example usage: test the first question\n",
    "#real_page, retrieved_pages = test_question(6)\n",
    "# Or test everything\n",
    "real_page, retrieved_pages = all_real_pages, all_retrieved_pages\n",
    "\n",
    "\n",
    "mrr = compute_mrr_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"MRR@{k}: {mrr:.4f}\")\n",
    "recall = compute_recall_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"Recall@{k}: {recall:.4f}\")\n",
    "precision = compute_precision_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"Precision@{k}: {precision:.4f}\")\n",
    "f1_score = compute_f1_score(retrieved_pages, real_page, k)\n",
    "print(f\"F1 Score@{k}: {f1_score:.4f}\")\n",
    "map_score = compute_map_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"mAP Score: {map_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Tuple\n",
    "\n",
    "def normalize_input(\n",
    "    retrieved: Union[List[List[str]], List[str]], \n",
    "    true_pages: Union[List[List[str]], List[str]]  # was `real`\n",
    ") -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    if isinstance(retrieved[0], str):  # Single query\n",
    "        return [list(map(str, retrieved))], [list(map(str, true_pages))]\n",
    "    else:\n",
    "        return (\n",
    "            [list(map(str, r)) for r in retrieved],\n",
    "            [list(map(str, r)) for r in true_pages]\n",
    "        )\n",
    "\n",
    "\n",
    "def compute_mrr_at_k(\n",
    "    all_retrieved_pages: Union[List[List[str]], List[str]], \n",
    "    all_real_pages: Union[List[List[str]], List[str]], \n",
    "    k: int\n",
    ") -> float:\n",
    "    all_retrieved_pages, all_real_pages = normalize_input(all_retrieved_pages, all_real_pages)\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for retrieved_pages, real_pages in zip(all_retrieved_pages, all_real_pages):\n",
    "        top_k_pages = retrieved_pages[:k]\n",
    "        rank = float('inf')\n",
    "\n",
    "        for real_page in real_pages:\n",
    "            if real_page in top_k_pages:\n",
    "                rank = min(rank, top_k_pages.index(real_page) + 1)  # 1-based\n",
    "\n",
    "        reciprocal_ranks.append(1.0 / rank if rank != float('inf') else 0.0)\n",
    "\n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "\n",
    "def compute_recall_at_k(\n",
    "    all_retrieved_pages: Union[List[List[str]], List[str]], \n",
    "    all_real_pages: Union[List[List[str]], List[str]], \n",
    "    k: int\n",
    ") -> float:\n",
    "    all_retrieved_pages, all_real_pages = normalize_input(all_retrieved_pages, all_real_pages)\n",
    "    total_hits = 0\n",
    "    total_relevant = 0\n",
    "\n",
    "    for retrieved_pages, real_pages in zip(all_retrieved_pages, all_real_pages): \n",
    "        top_k_pages = retrieved_pages[:k]\n",
    "        hits = sum(1 for page in real_pages if page in top_k_pages)\n",
    "        total_hits += hits\n",
    "        total_relevant += len(real_pages)\n",
    "\n",
    "    return total_hits / total_relevant if total_relevant else 0.0\n",
    "\n",
    "\n",
    "def compute_precision_at_k(\n",
    "    all_retrieved_pages: Union[List[List[str]], List[str]], \n",
    "    all_real_pages: Union[List[List[str]], List[str]], \n",
    "    k: int\n",
    ") -> float:\n",
    "    all_retrieved_pages, all_real_pages = normalize_input(all_retrieved_pages, all_real_pages)\n",
    "    total_hits = 0\n",
    "    total_retrieved = 0\n",
    "\n",
    "    for retrieved_pages, real_pages in zip(all_retrieved_pages, all_real_pages):\n",
    "        top_k_pages = retrieved_pages[:k]\n",
    "        hits = sum(1 for page in top_k_pages if page in real_pages)\n",
    "        total_hits += hits\n",
    "        total_retrieved += len(top_k_pages)\n",
    "\n",
    "    return total_hits / total_retrieved if total_retrieved else 0.0\n",
    "\n",
    "\n",
    "def compute_f1_score(\n",
    "    all_retrieved_pages: Union[List[List[str]], List[str]], \n",
    "    all_real_pages: Union[List[List[str]], List[str]], \n",
    "    k: int\n",
    ") -> float:\n",
    "    precision = compute_precision_at_k(all_retrieved_pages, all_real_pages, k)\n",
    "    recall = compute_recall_at_k(all_retrieved_pages, all_real_pages, k)\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_map_at_k(\n",
    "    all_retrieved_pages: Union[List[List[str]], List[str]], \n",
    "    all_real_pages: Union[List[List[str]], List[str]], \n",
    "    k: int\n",
    ") -> float:\n",
    "    all_retrieved_pages, all_real_pages = normalize_input(all_retrieved_pages, all_real_pages)\n",
    "    average_precisions = []\n",
    "\n",
    "    for retrieved_pages, real_pages in zip(all_retrieved_pages, all_real_pages):\n",
    "        top_k_pages = retrieved_pages[:k]\n",
    "        num_relevant = 0\n",
    "        precision_sum = 0.0\n",
    "\n",
    "        for i, page in enumerate(top_k_pages):\n",
    "            if page in real_pages:\n",
    "                num_relevant += 1\n",
    "                precision_sum += num_relevant / (i + 1)  # 1-based index\n",
    "\n",
    "        average_precisions.append(precision_sum / num_relevant if num_relevant > 0 else 0.0)\n",
    "\n",
    "    return sum(average_precisions) / len(average_precisions) if average_precisions else 0.0\n",
    "\n",
    "\n",
    "all_real_pages, all_retrieved_pages = [], []\n",
    "\n",
    "for rag_answer in rag_generated_answers:\n",
    "    real_page = next(\n",
    "        item[\"page_number\"] for item in qa_data if item[\"question\"] == rag_answer[\"query\"]\n",
    "    )\n",
    "    \n",
    "    #retrieved_pages = [doc[\"page_number\"] for doc in rag_answer[\"retrieved_contexts\"]]\n",
    "    retrieved_pages = [str(doc[\"page_number\"]) for doc in rag_answer[\"retrieved_contexts\"]]\n",
    "\n",
    "    all_real_pages.append([str(real_page)] if isinstance(real_page, (int, float)) else [str(p) for p in real_page])\n",
    "\n",
    "    #all_real_pages.append([real_page] if isinstance(real_page, int) else real_page)\n",
    "    all_retrieved_pages.append(retrieved_pages)\n",
    "\n",
    "# Function to test a specific question by index\n",
    "def test_question(index):\n",
    "    if index < 1 or index > len(rag_generated_answers):\n",
    "        print(\"Invalid index. Please select a number between 1 and 5.\")\n",
    "        return\n",
    "\n",
    "    rag_answer = rag_generated_answers[index - 1]\n",
    "    real_page = all_real_pages[index - 1]\n",
    "    retrieved_pages = all_retrieved_pages[index - 1]\n",
    "\n",
    "    print(f\"Question: {rag_answer['query']}\")\n",
    "    print(f\"True Answer: {rag_answer['true_answer']}\")\n",
    "    #print(f\"Generated Answer: {rag_answer['generated_answer']}\")\n",
    "    print(f\"Real Page(s): {real_page}\")\n",
    "    print(f\"Retrieved Pages: {retrieved_pages}\")\n",
    "    return real_page, retrieved_pages\n",
    "\n",
    "#Example usage: test the first question\n",
    "#real_page, retrieved_pages = test_question(6)\n",
    "# Or test everything\n",
    "real_page, retrieved_pages = all_real_pages, all_retrieved_pages\n",
    "\n",
    "\n",
    "mrr = compute_mrr_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"MRR@{k}: {mrr:.4f}\")\n",
    "recall = compute_recall_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"Recall@{k}: {recall:.4f}\")\n",
    "precision = compute_precision_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"Precision@{k}: {precision:.4f}\")\n",
    "f1_score = compute_f1_score(retrieved_pages, real_page, k)\n",
    "print(f\"F1 Score@{k}: {f1_score:.4f}\")\n",
    "map_score = compute_map_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"mAP Score: {map_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Config\n",
    "PDF_FILE = \"knowledge/riksbanken.pdf\"\n",
    "qa_filepath = \"json_files/QA_\" + os.path.basename(PDF_FILE).replace('.pdf', '.json')\n",
    "\n",
    "# Paths to different RAG result files\n",
    "rag_answer_paths = {\n",
    "    \"Txt sum (cmplx)\": \"json_files/rag_generated_answers_QA_riksbanken_dual_storage.json\",\n",
    "}\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5, 10]\n",
    "\n",
    "# Load QA data once\n",
    "with open(qa_filepath, 'r', encoding='utf-8') as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "# Prepare results storage\n",
    "results = {}\n",
    "\n",
    "# Loop through each RAG result file\n",
    "for name, path in rag_answer_paths.items():\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        rag_generated_answers = json.load(f)\n",
    "\n",
    "    all_real_pages, all_retrieved_pages = [], []\n",
    "\n",
    "    for rag_answer in rag_generated_answers:\n",
    "        real_page = next(\n",
    "            item[\"page_number\"] for item in qa_data if item[\"question\"] == rag_answer[\"query\"]\n",
    "        )\n",
    "        retrieved_pages = [str(doc[\"page_number\"]) for doc in rag_answer[\"retrieved_contexts\"]]\n",
    "\n",
    "        all_real_pages.append([str(real_page)] if isinstance(real_page, (int, float)) else [str(p) for p in real_page])\n",
    "        all_retrieved_pages.append(retrieved_pages)\n",
    "\n",
    "    results[name] = []\n",
    "    for k in k_values:\n",
    "        f1_at_k = compute_f1_score(all_retrieved_pages, all_real_pages, k)\n",
    "        results[name].append(f1_at_k)\n",
    "\n",
    "# Print results in a formatted table\n",
    "print(f\"{'Method':<25} | \" + \" | \".join([f\"F1@{k:<3}\" for k in k_values]))\n",
    "print(\"-\" * (27 + 9 * len(k_values)))\n",
    "\n",
    "for name, scores in results.items():\n",
    "    score_strs = [f\"{score:.4f}\" for score in scores]\n",
    "    print(f\"{name:<25} | \" + \" | \".join(score_strs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Batch evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the file path for storing the results\n",
    "results_folder = \"resultsbatch\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "# Function to evaluate in batches\n",
    "def evaluate_in_batches(rag_generated_answers, evaluator_llm, batch_size=25):\n",
    "    total_metrics = {\"faithful_rate\": 0.0, \"relevance_rate\": 0.0, \"answer_correctness\": 0.0}\n",
    "    total_batches = len(rag_generated_answers) // batch_size + (1 if len(rag_generated_answers) % batch_size > 0 else 0)\n",
    "    \n",
    "    for batch_index in range(total_batches):\n",
    "        # Define the batch's results file path\n",
    "        batch_file_path = os.path.join(results_folder, f\"batch_{batch_index+1}_{rag.name}.json\")\n",
    "        \n",
    "        # Check if the batch result already exists\n",
    "        if os.path.exists(batch_file_path):\n",
    "            print(f\"Batch {batch_index+1} results already exist: {batch_file_path}\")\n",
    "            with open(batch_file_path, 'r', encoding='utf-8') as file:\n",
    "                batch_results = json.load(file)\n",
    "                print(f\"Loaded batch {batch_index+1} results: {batch_results}\")\n",
    "        else:\n",
    "            # Create a subset of the answers for this batch\n",
    "            batch_start = batch_index * batch_size\n",
    "            batch_end = min((batch_index + 1) * batch_size, len(rag_generated_answers))\n",
    "            subset_rag_generated_answers = rag_generated_answers[batch_start:batch_end]\n",
    "            \n",
    "            # Evaluate the batch\n",
    "            faithfulness_and_relevance = str(evaluate_generation(subset_rag_generated_answers, evaluator_llm))\n",
    "            \n",
    "            # Replace single quotes with double quotes\n",
    "            json_string = faithfulness_and_relevance.replace(\"'\", '\"')\n",
    "            \n",
    "            # Save the batch results\n",
    "            with open(batch_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(json_string)  # Write string instead of using json.dump()\n",
    "            print(f\"Results for batch {batch_index+1} saved to: {batch_file_path}\")\n",
    "            \n",
    "            # Load the batch results into the dictionary\n",
    "            batch_results = json.loads(json_string)\n",
    "        \n",
    "        # Accumulate metrics for averaging\n",
    "        total_metrics[\"faithful_rate\"] += batch_results.get(\"faithful_rate\", 0)\n",
    "        total_metrics[\"relevance_rate\"] += batch_results.get(\"relevance_rate\", 0)\n",
    "        total_metrics[\"answer_correctness\"] += batch_results.get(\"answer_correctness\", 0)\n",
    "    \n",
    "    # Calculate the average of the metrics\n",
    "    total_batches = max(1, total_batches)  # Avoid division by zero in case of empty input\n",
    "    average_metrics = {\n",
    "        \"average_faithful_rate\": total_metrics[\"faithful_rate\"] / total_batches,\n",
    "        \"average_relevance_rate\": total_metrics[\"relevance_rate\"] / total_batches,\n",
    "        \"average_answer_correctness\": total_metrics[\"answer_correctness\"] / total_batches\n",
    "    }\n",
    "\n",
    "    return average_metrics\n",
    "\n",
    "# Example usage of the function\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming rag_generated_answers and evaluator_llm are already defined\n",
    "    subset_rag_generated_answers = rag_generated_answers[:1]  # Example subset\n",
    "\n",
    "    print(subset_rag_generated_answers)\n",
    "    average_metrics = evaluate_in_batches(subset_rag_generated_answers, evaluator_llm)\n",
    "    print(\"Final average metrics:\", average_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Evaluate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for storing the results\n",
    "results_folder = \"results\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "results_file_path = os.path.join(results_folder, os.path.basename(qa_filepath).replace('.json', f'_{rag.name}.json'))\n",
    "\n",
    "# Create a subset of 20 answers from rag_generated_answers\n",
    "subset_rag_generated_answers = rag_generated_answers[:1]\n",
    "# Check if the results file already exists\n",
    "if os.path.exists(results_file_path):\n",
    "    print(f\"Results already exist: {results_file_path}\")\n",
    "    print(\"Loading existing results...\")\n",
    "    with open(results_file_path, 'r', encoding='utf-8') as file:\n",
    "        print(file.read())\n",
    "    try:\n",
    "        with open(results_file_path, 'r', encoding='utf-8') as f:\n",
    "            faithfulness_and_relevance = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: The file {results_file_path} contains invalid JSON or is empty.\")\n",
    "        faithfulness_and_relevance = None\n",
    "else:\n",
    "    # Evaluate generation\n",
    "    #faithfulness_and_relevance = str(evaluate_generation_chart(rag_generated_answers, evaluator_llm))\n",
    "    faithfulness_and_relevance = str(evaluate_generation(subset_rag_generated_answers, evaluator_llm))\n",
    "    \n",
    "    # Replace single quotes with double quotes\n",
    "    json_string = faithfulness_and_relevance.replace(\"'\", '\"')\n",
    "\n",
    "    with open(results_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(json_string)  # Write string instead of using json.dump()\n",
    "    print(f\"Results saved to: {results_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
