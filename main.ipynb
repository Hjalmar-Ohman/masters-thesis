{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. INSTALL / IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "import openai\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CONFIGURE OPENAI & OTHER SETUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"openai/clip-vit-base-patch32\"  # can be changed to another CLIP model\n",
    "clip_model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create a cache dictionary in memory\n",
    "response_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts, processor, model):\n",
    "    \"\"\"\n",
    "    Given a list of text strings, return their CLIP embeddings as a NumPy array.\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_embeddings = model.get_text_features(**inputs)\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "    return text_embeddings.cpu().numpy()\n",
    "\n",
    "def embed_images(images, processor, model):\n",
    "    \"\"\"\n",
    "    Given a list of PIL images, return their CLIP embeddings as a NumPy array.\n",
    "    \"\"\"\n",
    "    inputs = processor(\n",
    "        images=images,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_embeddings = model.get_image_features(**inputs)\n",
    "        \n",
    "    # Normalize embeddings\n",
    "    image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "    return image_embeddings.cpu().numpy()\n",
    "\n",
    "def get_faiss_index(dimension):\n",
    "    \"\"\"\n",
    "    Create and return a flat Faiss index of the specified dimension.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexFlatIP(dimension)  # Using Inner Product (cosine similarity) \n",
    "    return index\n",
    "\n",
    "def add_to_index(index, embeddings):\n",
    "    \"\"\"\n",
    "    Add embeddings to a Faiss index.\n",
    "    \"\"\"\n",
    "    index.add(embeddings)\n",
    "\n",
    "def search_index(index, query_embedding, top_k=5):\n",
    "    \"\"\"\n",
    "    Search the Faiss index for the top_k nearest neighbors to query_embedding.\n",
    "    Returns (distances, indices).\n",
    "    \"\"\"\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return distances, indices\n",
    "\n",
    "def retrieve_context(indices, metadata):\n",
    "    \"\"\"\n",
    "    Given a list of indices from Faiss, return the corresponding metadata (text snippets, image descriptions, etc.).\n",
    "    \"\"\"\n",
    "    retrieved = []\n",
    "    for idx in indices[0]:\n",
    "        retrieved.append(metadata[idx])\n",
    "    return retrieved\n",
    "\n",
    "def call_gpt_4(prompt):\n",
    "    \"\"\"\n",
    "    Calls GPT-4 with a prompt and returns the response.\n",
    "    Caching is used so repeated prompts are not re-sent to the API.\n",
    "    \"\"\"\n",
    "    if prompt in response_cache:\n",
    "        return response_cache[prompt]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "    \n",
    "    # Store in cache\n",
    "    response_cache[prompt] = answer\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PROCESS THE PDF (TEXT + IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"knowledge/subset_monetary_policy_report.pdf\"\n",
    "\n",
    "text_data = []\n",
    "image_data = []\n",
    "\n",
    "# Extract text\n",
    "reader = PdfReader(PDF_FILE)\n",
    "num_pages = len(reader.pages)\n",
    "\n",
    "for page_i in range(num_pages):\n",
    "    page = reader.pages[page_i]\n",
    "    page_text = page.extract_text()\n",
    "    \n",
    "    # You might want to chunk the text if it's very long. Here we store the entire page text as one chunk.\n",
    "    if page_text and page_text.strip():\n",
    "        text_data.append({\n",
    "            \"text\": page_text.strip(),\n",
    "            \"page_number\": page_i + 1\n",
    "        })\n",
    "\n",
    "# Extract images\n",
    "# pdf2image.convert_from_path converts each PDF page into a PIL image \n",
    "pages_as_images = convert_from_path(PDF_FILE, dpi=200, poppler_path=r'poppler-24.08.0\\Library\\bin')  # You can adjust the dpi\n",
    "for i, pil_img in enumerate(pages_as_images):\n",
    "    image_data.append({\n",
    "        \"image\": pil_img,\n",
    "        \"page_number\": i + 1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CREATE EMBEDDINGS FOR TEXT AND IMAGE CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metadata = []\n",
    "all_embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5A. Embed all text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_list = [td[\"text\"] for td in text_data]\n",
    "if len(texts_list) > 0:\n",
    "    text_embeddings = embed_texts(texts_list, clip_processor, clip_model)\n",
    "    for i, emb in enumerate(text_embeddings):\n",
    "        all_metadata.append({\n",
    "            \"type\": \"text\",\n",
    "            \"content\": text_data[i][\"text\"],\n",
    "            \"page_number\": text_data[i][\"page_number\"]\n",
    "        })\n",
    "        all_embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5B. Embed all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5B. Embed all images\n",
    "pil_images_list = [id_[\"image\"] for id_ in image_data]\n",
    "if len(pil_images_list) > 0:\n",
    "    image_embeddings = embed_images(pil_images_list, clip_processor, clip_model)\n",
    "    for i, emb in enumerate(image_embeddings):\n",
    "        all_metadata.append({\n",
    "            \"type\": \"image\",\n",
    "            \"content\": f\"Image from page {image_data[i]['page_number']}\",  # or store the actual PIL object if needed\n",
    "            \"page_number\": image_data[i][\"page_number\"]\n",
    "        })\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "# Convert to NumPy array\n",
    "all_embeddings = np.array(all_embeddings).astype('float32')\n",
    "embedding_dimension = all_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. BUILD & POPULATE FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = get_faiss_index(embedding_dimension)\n",
    "add_to_index(index, all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. QUERY PIPELINE (RETRIEVAL + GENERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(user_query, top_k=3):\n",
    "    \"\"\"\n",
    "    1. Embed the user query (assuming it's text).\n",
    "    2. Retrieve top_k similar items from the PDF (text or image).\n",
    "    3. Create a prompt for GPT-4 with the retrieved context.\n",
    "    4. Return GPT-4's answer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Embed user query as text\n",
    "    query_emb = embed_texts([user_query], clip_processor, clip_model)  # shape: (1, D)\n",
    "    \n",
    "    # Step 2: Retrieve from Faiss\n",
    "    distances, indices = search_index(index, query_emb, top_k=top_k)\n",
    "    retrieved_items = retrieve_context(indices, all_metadata)\n",
    "    \n",
    "    # Build a context string. You may want more sophisticated formatting.\n",
    "    context_str_list = []\n",
    "    for item in retrieved_items:\n",
    "        context_str_list.append(\n",
    "            f\"({item['type']}, page {item['page_number']}): {item['content'][:500]}...\"  # truncate if needed\n",
    "        )\n",
    "    context_str = \"\\n\".join(context_str_list)\n",
    "    \n",
    "    # Step 3: Create a prompt for GPT-4\n",
    "    prompt = f\"\"\"\n",
    "The user asked: \"{user_query}\"\n",
    "\n",
    "I have the following context from the PDF:\n",
    "{context_str}\n",
    "\n",
    "Based on the context above (and only this context if possible), answer the query:\n",
    "\"\"\"\n",
    "    \n",
    "    # Step 4: Call GPT-4\n",
    "    answer = call_gpt_4(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. EXAMPLE USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_1 = \"Give me a summary of the content in the PDF.\"\n",
    "response_1 = answer_query(user_query_1, top_k=3)\n",
    "print(\"Q:\", user_query_1)\n",
    "print(\"A:\", response_1)\n",
    "\n",
    "# Try the same query again and see if caching returns the same answer instantly.\n",
    "response_2 = answer_query(user_query_1, top_k=3)\n",
    "print(\"Q:\", user_query_1, \"(second time)\")\n",
    "print(\"A:\", response_2)\n",
    "\n",
    "# Another example query\n",
    "user_query_2 = \"Describe the images found on page 2.\"\n",
    "response_3 = answer_query(user_query_2, top_k=3)\n",
    "print(\"\\nQ:\", user_query_2)\n",
    "print(\"A:\", response_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
