{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from datasets import load_dataset\n",
    "\n",
    "from document_processor import TextProcessor, ImageProcessor, PageImageProcessor, ImageTextualSummaryProcessor\n",
    "from multimodal_rag import MultimodalRAG\n",
    "from embedder import OpenAIEmbedder, ColPaliEmbedder\n",
    "from pdf_to_qa import generate_qa_for_pdf, generate_chartQA_pdf_and_json\n",
    "from evaluation import evaluate_generation, compute_mrr_at_k, compute_recall_at_k, compute_precision_at_k, compute_f1_score, compute_map_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"CoinQA\"\n",
    "k = 10\n",
    "\n",
    "if dataset.upper() == \"COINQA\":\n",
    "    PDF_FILE = \"knowledge/subset_riksbanken.pdf\"\n",
    "elif dataset.upper() == \"CHARTQA\":\n",
    "    PDF_FILE = \"knowledge/subset_ChartQA.pdf\"\n",
    "else:\n",
    "    raise ValueError(\"Dataset not supported\")\n",
    "\n",
    "\n",
    "#text_processor = TextProcessor(OpenAIEmbedder())\n",
    "#image_processor = ImageProcessor(ColPaliEmbedder(), dataset)\n",
    "#page_image_processor = PageImageProcessor(ColPaliEmbedder())\n",
    "image_textual_summary_processor = ImageTextualSummaryProcessor(OpenAIEmbedder(), dataset)\n",
    "\n",
    "rag = MultimodalRAG([image_textual_summary_processor], PDF_FILE)\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\")) # For Ragas evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing QA file: json_files/QA_subset_riksbanken.json\n"
     ]
    }
   ],
   "source": [
    "# Check if QA file already exists\n",
    "qa_filepath = \"json_files/QA_\" + os.path.basename(PDF_FILE).replace('.pdf', '.json')\n",
    "\n",
    "if os.path.exists(qa_filepath):\n",
    "    print(f\"Using existing QA file: {qa_filepath}\")\n",
    "\n",
    "elif dataset.upper() == \"COINQA\":\n",
    "    generate_qa_for_pdf(PDF_FILE, json_output_path=qa_filepath)\n",
    "    print(f\"Generated new CoinQA file: {qa_filepath}\")\n",
    "\n",
    "elif dataset.upper() == \"CHARTQA\":\n",
    "    chartqa = load_dataset('lmms-lab/ChartQA', split='test')\n",
    "    subset_chartqa = chartqa.select(range(20))\n",
    "    \n",
    "    generate_chartQA_pdf_and_json(subset_chartqa, pdf_output_path=PDF_FILE, json_output_path=qa_filepath)\n",
    "    print(f\"Generated new ChartQA file: {qa_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Answering the QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing RAG generated answers file: json_files/rag_generated_answers_QA_subset_riksbanken.json\n"
     ]
    }
   ],
   "source": [
    "with open(qa_filepath, 'r', encoding='utf-8') as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "# Generate dataset\n",
    "rag_generated_answers = []\n",
    "\n",
    "# Check if generated answers file already exists\n",
    "rag_answers_path = \"json_files/rag_generated_answers_\" + os.path.basename(qa_filepath)\n",
    "\n",
    "if os.path.exists(rag_answers_path):\n",
    "    rag_generated_answers = json.load(open(rag_answers_path, 'r', encoding='utf-8'))\n",
    "    print(f\"Using existing RAG generated answers file: {rag_answers_path}\")\n",
    "else:\n",
    "    for qa in qa_data:\n",
    "        query = qa[\"question\"]\n",
    "        reference = qa[\"answer\"]\n",
    "\n",
    "        relevant_docs = rag.get_most_relevant_docs(query, top_k=k)\n",
    "        response = rag.generate_answer(query, relevant_docs)\n",
    "        rag_generated_answers.append(\n",
    "            {\n",
    "                \"query\":query,\n",
    "                \"retrieved_contexts\":relevant_docs,\n",
    "                \"generated_answer\":response,\n",
    "                \"true_answer\":reference\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Save the dataset to a JSON file\n",
    "    output_dataset_file = \"json_files/rag_generated_answers_\" + os.path.basename(qa_filepath)\n",
    "\n",
    "    with open(output_dataset_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(rag_generated_answers, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Generated new RAG generated answers file: {output_dataset_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How have the average mortgage rates on new loans changed since the beginning of 2024 compared to the Riksbank's policy rate?\n",
      "True Answer: The average mortgage rates on new loans have fallen by about as much as the Riksbank has cut its policy rate since the beginning of 2024.\n",
      "Generated Answer: Since the beginning of 2024, average mortgage rates on new loans have generally decreased, reflecting shifts in the broader economic environment and changes in the Riksbank's policy rate. The Riksbank has implemented cuts to its policy rate, which typically influences lending rates, including mortgage rates. However, the decline in mortgage rates has not been as pronounced as the decrease in the policy rate. \n",
      "\n",
      "This trend indicates that while banks may be lowering their rates in response to the policy changes, heightened competition for deposits and other financial instruments has impacted the extent to which mortgage rates can decrease. As a result, the average mortgage rates for new loans have shown a downward trend, but they remain higher than the cuts in the Riksbank's policy rate.\n",
      "\n",
      "In summary, average mortgage rates have decreased since early 2024, but not as significantly as the Riksbank's policy rate, due in part to competitive pressures in the financial market.\n",
      "Real Page(s): [5]\n",
      "Retrieved Pages: [6, 5, 7, 4, 8, 6, 3, 3, 11, 10]\n",
      "MRR@10: 0.50\n",
      "Recall@10: 1.00\n",
      "Precision@10: 0.10\n",
      "F1 Score@10: 0.18\n",
      "mAP Score: 0.50\n"
     ]
    }
   ],
   "source": [
    "all_real_pages, all_retrieved_pages = [], []\n",
    "\n",
    "for rag_answer in rag_generated_answers:\n",
    "    real_page = next(\n",
    "        item[\"page_number\"] for item in qa_data if item[\"question\"] == rag_answer[\"query\"]\n",
    "    )\n",
    "    retrieved_pages = [doc[\"page_number\"] for doc in rag_answer[\"retrieved_contexts\"]]\n",
    "    all_real_pages.append([real_page] if isinstance(real_page, int) else real_page)\n",
    "    all_retrieved_pages.append(retrieved_pages)\n",
    "\n",
    "# Function to test a specific question by index\n",
    "def test_question(index):\n",
    "    if index < 1 or index > len(rag_generated_answers):\n",
    "        print(\"Invalid index. Please select a number between 1 and 5.\")\n",
    "        return\n",
    "\n",
    "    rag_answer = rag_generated_answers[index - 1]\n",
    "    real_page = all_real_pages[index - 1]\n",
    "    retrieved_pages = all_retrieved_pages[index - 1]\n",
    "\n",
    "    print(f\"Question: {rag_answer['query']}\")\n",
    "    print(f\"True Answer: {rag_answer['true_answer']}\")\n",
    "    print(f\"Generated Answer: {rag_answer['generated_answer']}\")\n",
    "    print(f\"Real Page(s): {real_page}\")\n",
    "    print(f\"Retrieved Pages: {retrieved_pages}\")\n",
    "    return real_page, retrieved_pages\n",
    "\n",
    "# Example usage: test the first question\n",
    "#real_page, retrieved_pages = test_question(5)\n",
    "\n",
    "# Or test everything\n",
    "real_page, retrieved_pages = all_real_pages, all_retrieved_pages\n",
    "\n",
    "mrr = compute_mrr_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"MRR@{k}: {mrr:.2f}\")\n",
    "recall = compute_recall_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"Recall@{k}: {recall:.2f}\")\n",
    "precision = compute_precision_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"Precision@{k}: {precision:.2f}\")\n",
    "f1_score = compute_f1_score(retrieved_pages, real_page, k)\n",
    "print(f\"F1 Score@{k}: {f1_score:.2f}\")\n",
    "map = compute_map_at_k(retrieved_pages, real_page, k)\n",
    "print(f\"mAP Score: {map:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_and_relevance = evaluate_generation(rag_generated_answers, evaluator_llm)\n",
    "print(faithfulness_and_relevance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
